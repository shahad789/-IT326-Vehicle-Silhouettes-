---
title: "Vehicle silhouette"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

## Goal of collecting this dataset:

The goal of collecting this dataset is to determine the category of a
given vehicle silhouette among three types (car, van, bus) by utilizing
a set of extracted silhouette-based features.

## Classification:

The classification of vehicle silhouettes involves the utilization of a
set of distinctive attributes extracted from these silhouettes. The
primary objective is to categorize these silhouettes into one of three
predefined classes: cars, vans, or buses.

## Clustering:

Clustering vehicle silhouettes entails organizing similar data based on
their shared characteristics to create distinct clusters. Various
methods will be utilized to determine the number of clusters that best
suit our dataset, followed by evaluation techniques to assess the
effectiveness of these clusters in segmenting the data effectively based
on their inherent features.

## Defect prediction:

Defect prediction in a dataset of vehicle silhouettes involves spotting
potential issues or irregularities within these silhouette images. This
process may encompass the detection of inconsistencies or abnormalities
in the silhouettes, including the presence of missing data points, which
can be problematic. Leveraging data mining techniques, defect prediction
enhances the dataset's overall quality and dependability, ensuring it
faithfully reflects the desired information. This, in turn, reduces the
risk of producing inaccurate or deceptive outcomes when the dataset is
applied in subsequent tasks and applications, such as object recognition
and classification.

##### Link Of dataset: [link of dataset](https://www.kaggle.com/datasets/pritech/vehicle-silhouettes/data)

##### Number of attribute: 19

##### Number of objects: 847

```{r}
##First lets read the csv 
dataset<-read.csv('vehicle.csv')
databeforereprossess<-read.csv('vehicle.csv')
```

## Let us dive into our dataset to undertsand it more:

## A.Before preprocessing, Understanding the dataset

```{r}
head(dataset)
```

Reviewing a section of our dataset, it's evident that there are missing
values, and significant ranges in certain data, and our dataset
comprises numeric values to predict our categorical class label.

```{r}
str(dataset)
```

As we can see our dataset consists of 18 integer columns primarily
designed for classification purposes, aiding in the distinction of
various vehicle types such as cars, vans, and buses. These 18
integer-based metrics provide a detailed perspective on diverse vehicle
attributes, enabling precise and reliable classification.

```{r}
##Lets check summary of our attributes
summary(dataset)
```

The dataset includes 846 records and 19 attributes, aiming to classify
vehicle silhouettes as 'bus', 'car', or 'van'. These attributes,
numerically expressed, capture distinct silhouette features of the
vehicles. An analysis of the data shows that certain attributes such as
'scaled_variance.1', 'scaled_radius_of_gyration', and
'scaled_radius_of_gyration.1' exhibit a broader range and relatively
higher maximum values compared to other features. Moreover,
'pr.axis_rectangularity' and 'max.length_rectangularity' seem to have a
connection with the vehicle's compactness. There are isolated outliers
present in specific attributes that could potentially impact the
analysis.

The categorical variable 'class' enumerates vehicle classifications,
noting counts of observations for 'bus' (218), 'car' (429), and 'van'
(199). The notably high count of 'car' compared to the other categories
indicates a dataset imbalance. This dataset holds promise for predictive
modeling to categorize vehicles based on their silhouette attributes.
However, it requires cleaning and preprocessing before model development
to ensure more accurate and dependable predictions.

```{r}
sum(duplicated(dataset))
```

The result of '0' signifies that within the dataset, there are no rows
that are exact replicas of one another. Essentially, each row in the
dataset is distinct, ensuring that there are no identical rows present.

```{r}
attributes(dataset)
##where class is our class label
```

The data contains 19 variables including 'compactness', 'circularity',
'distance_circularity', 'radius_ratio', 'pr.axis_aspect_ratio',
'max.length_aspect_ratio', 'scatter_ratio', 'elongatedness',
'pr.axis_rectangularity', 'max.length_rectangularity',
'scaled_variance', 'scaled_variance.1', 'scaled_radius_of_gyration',
'scaled_radius_of_gyration.1', 'skewness_about', 'skewness_about.1',
'skewness_about.2', 'hollows_ratio', and 'class'. The 'class' variable
appears to be categorical with 846 observations, as indicated by the
range of row numbers from 1 to 846. This suggests the dataset is
structured and comprehensive.

###### Checking missing values and nulls

```{r}
##checking null values
is.na(dataset)
##checking for null values
sum(is.na(dataset))
```

###### Checking correlation and covariance of some attributes

```{r}
##lets look at some correlations and coveriance 
cov(dataset$compactness, dataset$hollows_ratio) 
cor(dataset$compactness, dataset$hollows_ratio) 

cov(dataset$max.length_aspect_ratio, dataset$hollows_ratio) 
cor(dataset$max.length_aspect_ratio, dataset$hollows_ratio)
```

The is a correlation between compactness and hollows_ratio but a weak
one, which means that we cant delete any of them. There is a correlation
as well in max length and hollows ratio but its also a weak correlation
Both pairs have a high coveriance

After having some information about our dataset lets present some visual
for more explanation

```{r}
library(ggplot2)

ggplot(dataset, aes(x = class)) + geom_bar(fill = "skyblue", color = "black") + labs(title = "Class Distribution Plot", x = "class", y = "Count")
```

After analyzing the dataset, we have observed that the distribution of
the 'class' variable is unbalanced, with some classes having
significantly fewer instances than others. To address this class
imbalance issue, we will implement techniques to rebalance the dataset
and ensure that the machine learning models can make more accurate
predictions for all classes.

```{r}
hist(dataset$max.length_aspect_ratio)
```

The histogram shows the freguency of the max length aspect ratio which
varies widely depending on the type and purpose of the vehicle which
will help us in categorizing it.


B.Preposseing stage


```{r}
###Encoding categorical data
dataset$class = factor(dataset$class,levels = c("bus","car", "van"), labels = c(1, 2, 3))
##Dealing with  missing value
dataset$compactness[is.na(dataset$compactness)]<- (median(dataset$compactness, na.rm =TRUE))
dataset$circularity[is.na(dataset$circularity)]<- (median(dataset$circularity, na.rm =TRUE))
dataset$distance_circularity[is.na(dataset$distance_circularity)]<- (median(dataset$distance_circularity, na.rm =TRUE))
dataset$radius_ratio[is.na(dataset$radius_ratio)]<- (median(dataset$radius_ratio, na.rm =TRUE))
dataset$pr.axis_aspect_ratio[is.na(dataset$pr.axis_aspect_ratio)]<- (median(dataset$pr.axis_aspect_ratio, na.rm =TRUE))
dataset$max.length_aspect_ratio[is.na(dataset$max.length_aspect_ratio)]<- (median(dataset$max.length_aspect_ratio, na.rm =TRUE))
dataset$scatter_ratio[is.na(dataset$scatter_ratio)]<- (median(dataset$scatter_ratio, na.rm =TRUE))
dataset$elongatedness[is.na(dataset$elongatedness)]<- (median(dataset$elongatedness, na.rm =TRUE))
dataset$pr.axis_rectangularity[is.na(dataset$pr.axis_rectangularity)]<- (median(dataset$pr.axis_rectangularity, na.rm =TRUE))
dataset$max.length_rectangularity[is.na(dataset$max.length_rectangularity)]<- (median(dataset$max.length_rectangularity, na.rm =TRUE))
dataset$scaled_variance[is.na(dataset$scaled_variance)]<- (median(dataset$scaled_variance, na.rm =TRUE))
dataset$scaled_variance.1[is.na(dataset$scaled_variance.1)]<- (median(dataset$scaled_variance.1, na.rm =TRUE))
dataset$scaled_radius_of_gyration[is.na(dataset$scaled_radius_of_gyration)]<- (median(dataset$scaled_radius_of_gyration, na.rm =TRUE))
dataset$scaled_radius_of_gyration.1[is.na(dataset$scaled_radius_of_gyration.1)]<- (median(dataset$scaled_radius_of_gyration.1, na.rm =TRUE))
dataset$skewness_about[is.na(dataset$skewness_about)]<- (median(dataset$skewness_about, na.rm =TRUE))
dataset$skewness_about.1[is.na(dataset$skewness_about.1)]<- (median(dataset$skewness_about.1, na.rm =TRUE))
dataset$skewness_about.2[is.na(dataset$skewness_about.2)]<- (median(dataset$skewness_about.2, na.rm =TRUE))
dataset$hollows_ratio[is.na(dataset$hollows_ratio)]<- (median(dataset$hollows_ratio, na.rm =TRUE))


## outliers analysis

##radius_ratio founded outliers 3

quartiles <- quantile(dataset$radius_ratio, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(dataset$radius_ratio)

Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 

data_no_outlier <- subset(dataset, dataset$radius_ratio > Lower & dataset$radius_ratio < Upper)
dim(data_no_outlier)

##pr.axis_aspect_ratio outliers 8

quartiles <- quantile(dataset$pr.axis_aspect_ratio, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(dataset$pr.axis_aspect_ratio)

Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 

data_no_outlier <- subset(dataset, dataset$pr.axis_aspect_ratio > Lower & dataset$pr.axis_aspect_ratio < Upper)
dim(data_no_outlier)

##max.length_aspect_ratio outliers is 5

quartiles <- quantile(dataset$max.length_aspect_ratio, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(dataset$max.length_aspect_ratio)

Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 

data_no_outlier <- subset(dataset, dataset$max.length_aspect_ratio > Lower & dataset$max.length_aspect_ratio < Upper)
dim(data_no_outlier)

##scaled_variance outliers is 1

quartiles <- quantile(dataset$scaled_variance, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(dataset$scaled_variance)

Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 

data_no_outlier <- subset(dataset, dataset$scaled_variance > Lower & dataset$scaled_variance < Upper)
dim(data_no_outlier)

##scaled_variance.1 outliers is 2

quartiles <- quantile(dataset$scaled_variance.1, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(dataset$scaled_variance.1)

Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 

data_no_outlier <- subset(dataset, dataset$scaled_variance.1 > Lower & dataset$scaled_variance.1 < Upper)
dim(data_no_outlier)


##scaled_radius_of_gyration.1 is outliers is 10

quartiles <- quantile(dataset$scaled_radius_of_gyration.1, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(dataset$scaled_variance.1)

Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 

data_no_outlier <- subset(dataset, dataset$scaled_radius_of_gyration.1 > Lower & dataset$scaled_radius_of_gyration.1 < Upper)
dim(data_no_outlier)


##skewness_about is outliers is 3

quartiles <- quantile(dataset$skewness_about, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(dataset$skewness_about)

Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 

data_no_outlier <- subset(dataset, dataset$skewness_about > Lower & dataset$skewness_about < Upper)
dim(data_no_outlier)


##skewness_about.1 is outliers is 3

quartiles <- quantile(dataset$skewness_about.1, probs=c(.25, .75), na.rm = FALSE)
IQR <- IQR(dataset$skewness_about.1)

Lower <- quartiles[1] - 1.5*IQR
Upper <- quartiles[2] + 1.5*IQR 

data_no_outlier <- subset(dataset, dataset$skewness_about.1 > Lower & dataset$skewness_about.1 < Upper)
dim(data_no_outlier)

##feature selection
cor(dataset[, unlist(lapply(dataset, is.numeric))]) 
dataset<- subset(dataset, select = -pr.axis_rectangularity)
dataset<- subset(dataset, select = -compactness)
##normalization
normalize <- function(x) {return((x-min(x))/ (max(x)-min(x)))}
dataset$scaled_variance.1 <-normalize(dataset$scaled_variance.1)
```





